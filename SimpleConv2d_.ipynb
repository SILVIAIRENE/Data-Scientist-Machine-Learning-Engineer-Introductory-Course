{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7pvAljuw/+XCa7PxcRARY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SILVIAIRENE/Data-Scientist-Machine-Learning-Engineer-Introductory-Course/blob/master/SimpleConv2d_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVy0PVxwk7a7",
        "outputId": "528bd653-768b-4988-ae9f-04177af0aac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ejecutando verificación Problema 2 ===\n",
            "Problema 2 - Forward output:\n",
            " [[[[-4 -4]\n",
            "   [-4 -4]]\n",
            "\n",
            "  [[ 1  1]\n",
            "   [ 1  1]]]]\n",
            "Problema 2 - Backward dx (padded removed):\n",
            " [[  0   0   0   0]\n",
            " [  0  -5   4  -7]\n",
            " [  0  13  27 -11]\n",
            " [  0 -10 -11   0]]\n",
            "=== Fin verificación Problema 2 ===\n",
            "\n",
            "\n",
            "=== Cargando MNIST y entrenando LeNet pequeño (subset) ===\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/1 - loss: 2.3747 - acc: 12.50% - time: 229.3s\n",
            "Test accuracy (first 500 samples): 16.60%\n",
            "=== Fin entrenamiento y evaluación rápida ===\n",
            "\n",
            "\n",
            "=== Problema 10: Tamaño de salida y número de parámetros ===\n",
            "Input: 144x144, in_ch=3, filter=(3, 3), out_ch=6, stride=1\n",
            " -> Output size: 142 x 142; Num params (incl. bias): 168\n",
            "Input: 60x60, in_ch=24, filter=(3, 3), out_ch=48, stride=1\n",
            " -> Output size: 58 x 58; Num params (incl. bias): 10416\n",
            "Input: 20x20, in_ch=10, filter=(3, 3), out_ch=20, stride=2\n",
            " -> Output size: 9 x 9; Num params (incl. bias): 1820\n",
            "=== Fin Problema 10 ===\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Red Neuronal Convolucional 2D Scratch - Solución completa (lista para Google Colab)\n",
        "# Autor: Respuesta generada por un Ingeniero ML experto (implementación NumPy)\n",
        "# NOTA: Este script usa únicamente NumPy y tf.keras.datasets para cargar MNIST.\n",
        "# Ejecuta en Google Colab: asegúrate de tener suficiente RAM; el entrenamiento aquí es pequeño por rapidez.\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 0] Imports y utilidades\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from time import time\n",
        "\n",
        "# Reproducibilidad rápida\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 3] Función: tamaño de salida después de la convolución 2D\n",
        "# Fórmula:\n",
        "# N_out_h = (N_in_h + 2*PAD_h - F_h) // S_h + 1\n",
        "# N_out_w = (N_in_w + 2*PAD_w - F_w) // S_w + 1\n",
        "# Implementamos una función utilitaria.\n",
        "# ----------------------------\n",
        "def conv2d_output_size(n_in_h, n_in_w, filter_h, filter_w, pad_h=0, pad_w=0, stride_h=1, stride_w=1):\n",
        "    n_out_h = (n_in_h + 2*pad_h - filter_h) // stride_h + 1\n",
        "    n_out_w = (n_in_w + 2*pad_w - filter_w) // stride_w + 1\n",
        "    return n_out_h, n_out_w\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 1] Clase Conv2d (NCHW)\n",
        "# Implementa forward y backward (grad w.r.t weights, bias y entrada).\n",
        "# Soporta padding y stride (integers or tuples).\n",
        "# ----------------------------\n",
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        # kernel_size can be int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kh, kw = kernel_size, kernel_size\n",
        "        else:\n",
        "            kh, kw = kernel_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kh = kh\n",
        "        self.kw = kw\n",
        "        if isinstance(stride, int):\n",
        "            self.stride_h = self.stride_w = stride\n",
        "        else:\n",
        "            self.stride_h, self.stride_w = stride\n",
        "        if isinstance(padding, int):\n",
        "            self.pad_h = self.pad_w = padding\n",
        "        else:\n",
        "            self.pad_h, self.pad_w = padding\n",
        "        # He initialization for conv filters\n",
        "        fan_in = in_channels * kh * kw\n",
        "        limit = np.sqrt(2.0 / fan_in)\n",
        "        self.W = np.random.randn(out_channels, in_channels, kh, kw) * limit\n",
        "        self.b = np.zeros((out_channels,), dtype=np.float32)\n",
        "\n",
        "        # placeholders for backward\n",
        "        self.cache = None\n",
        "        # gradients\n",
        "        self.dW = np.zeros_like(self.W)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, C, H, W) NCHW\n",
        "        N, C, H, W = x.shape\n",
        "        assert C == self.in_channels, \"Input channel mismatch\"\n",
        "        out_h, out_w = conv2d_output_size(H, W, self.kh, self.kw, self.pad_h, self.pad_w, self.stride_h, self.stride_w)\n",
        "        y = np.zeros((N, self.out_channels, out_h, out_w), dtype=np.float32)\n",
        "\n",
        "        # pad input\n",
        "        x_padded = np.pad(x, ((0,0),(0,0),(self.pad_h,self.pad_h),(self.pad_w,self.pad_w)), mode='constant')\n",
        "        # naive loops (clear and correct)\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        patch = x_padded[n, :, h_start:h_start + self.kh, w_start:w_start + self.kw]\n",
        "                        y[n, m, i, j] = np.sum(patch * self.W[m]) + self.b[m]\n",
        "        self.cache = (x, x_padded, y.shape)\n",
        "        return y\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        # d_out: (N, out_channels, out_h, out_w)\n",
        "        x, x_padded, out_shape = self.cache\n",
        "        N, C, H, W = x.shape\n",
        "        _, _, out_h, out_w = d_out.shape\n",
        "\n",
        "        # initialize gradients\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dx_padded = np.zeros_like(x_padded)\n",
        "\n",
        "        # compute dW and db and dx_padded\n",
        "        for n in range(N):\n",
        "            for m in range(self.out_channels):\n",
        "                db[m] += np.sum(d_out[n, m])\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        patch = x_padded[n, :, h_start:h_start + self.kh, w_start:w_start + self.kw]\n",
        "                        dW[m] += d_out[n, m, i, j] * patch\n",
        "                        dx_padded[n, :, h_start:h_start + self.kh, w_start:w_start + self.kw] += d_out[n, m, i, j] * self.W[m]\n",
        "\n",
        "        # remove padding from dx_padded\n",
        "        if self.pad_h == 0 and self.pad_w == 0:\n",
        "            dx = dx_padded\n",
        "        else:\n",
        "            dx = dx_padded[:, :, self.pad_h:self.pad_h+H, self.pad_w:self.pad_w+W]\n",
        "\n",
        "        # store gradients\n",
        "        self.dW = dW\n",
        "        self.db = db\n",
        "        return dx\n",
        "\n",
        "    def step(self, lr=1e-3):\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 4] Clase MaxPool2D (NCHW)\n",
        "# Guarda índices de max para el backward\n",
        "# ----------------------------\n",
        "class MaxPool2D:\n",
        "    def __init__(self, kernel_size=2, stride=None):\n",
        "        if isinstance(kernel_size, int):\n",
        "            self.kh = self.kw = kernel_size\n",
        "        else:\n",
        "            self.kh, self.kw = kernel_size\n",
        "        if stride is None:\n",
        "            self.stride_h = self.kh\n",
        "            self.stride_w = self.kw\n",
        "        else:\n",
        "            if isinstance(stride, int):\n",
        "                self.stride_h = self.stride_w = stride\n",
        "            else:\n",
        "                self.stride_h, self.stride_w = stride\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, C, H, W)\n",
        "        N, C, H, W = x.shape\n",
        "        out_h, out_w = conv2d_output_size(H, W, self.kh, self.kw, 0, 0, self.stride_h, self.stride_w)\n",
        "        y = np.zeros((N, C, out_h, out_w), dtype=x.dtype)\n",
        "        # mask to store indices\n",
        "        mask = np.zeros_like(x, dtype=bool)\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        patch = x[n, c, h_start:h_start + self.kh, w_start:w_start + self.kw]\n",
        "                        flat_index = np.argmax(patch)\n",
        "                        # store max\n",
        "                        y[n, c, i, j] = patch.flatten()[flat_index]\n",
        "                        # convert flat_index to 2d\n",
        "                        ph = flat_index // self.kw\n",
        "                        pw = flat_index % self.kw\n",
        "                        mask[n, c, h_start + ph, w_start + pw] = True\n",
        "        self.cache = (x.shape, mask)\n",
        "        return y\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        x_shape, mask = self.cache\n",
        "        N, C, H, W = x_shape\n",
        "        dx = np.zeros(x_shape, dtype=np.float32)\n",
        "        out_h, out_w = d_out.shape[2], d_out.shape[3]\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        # find where mask True inside window and add gradient\n",
        "                        window_mask = mask[n, c, h_start:h_start + self.kh, w_start:w_start + self.kw]\n",
        "                        # there should be exactly one True\n",
        "                        dx[n, c, h_start:h_start + self.kh, w_start:w_start + self.kw] += d_out[n, c, i, j] * window_mask\n",
        "        return dx\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 5] AveragePool2D (opcional avanzado)\n",
        "# ----------------------------\n",
        "class AveragePool2D:\n",
        "    def __init__(self, kernel_size=2, stride=None):\n",
        "        if isinstance(kernel_size, int):\n",
        "            self.kh = self.kw = kernel_size\n",
        "        else:\n",
        "            self.kh, self.kw = kernel_size\n",
        "        if stride is None:\n",
        "            self.stride_h = self.kh\n",
        "            self.stride_w = self.kw\n",
        "        else:\n",
        "            if isinstance(stride, int):\n",
        "                self.stride_h = self.stride_w = stride\n",
        "            else:\n",
        "                self.stride_h, self.stride_w = stride\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h, out_w = conv2d_output_size(H, W, self.kh, self.kw, 0, 0, self.stride_h, self.stride_w)\n",
        "        y = np.zeros((N, C, out_h, out_w), dtype=x.dtype)\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        patch = x[n, c, h_start:h_start + self.kh, w_start:w_start + self.kw]\n",
        "                        y[n, c, i, j] = np.mean(patch)\n",
        "        self.cache = (x.shape,)\n",
        "        return y\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        x_shape = self.cache[0]\n",
        "        N, C, H, W = x_shape\n",
        "        dx = np.zeros(x_shape, dtype=np.float32)\n",
        "        out_h, out_w = d_out.shape[2], d_out.shape[3]\n",
        "        area = self.kh * self.kw\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "                        dx[n, c, h_start:h_start + self.kh, w_start:w_start + self.kw] += d_out[n, c, i, j] / area\n",
        "        return dx\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 6] Flatten\n",
        "# ----------------------------\n",
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        self.orig_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, C, H, W) -> (N, C*H*W)\n",
        "        self.orig_shape = x.shape\n",
        "        N = x.shape[0]\n",
        "        return x.reshape(N, -1)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        return d_out.reshape(self.orig_shape)\n",
        "\n",
        "# ----------------------------\n",
        "# Capas Fully connected y Activaciones (utilidades)\n",
        "# ----------------------------\n",
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
        "        self.b = np.zeros((out_features,), dtype=np.float32)\n",
        "        self.cache = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, D)\n",
        "        self.cache = x\n",
        "        return x.dot(self.W) + self.b\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        x = self.cache  # (N, D)\n",
        "        self.dW = x.T.dot(d_out)  # (D, out)\n",
        "        self.db = np.sum(d_out, axis=0)\n",
        "        dx = d_out.dot(self.W.T)\n",
        "        return dx\n",
        "\n",
        "    def step(self, lr=1e-3):\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_backward(x, d_out):\n",
        "    dx = d_out.copy()\n",
        "    dx[x <= 0] = 0\n",
        "    return dx\n",
        "\n",
        "def softmax(x):\n",
        "    # x: (N, C)\n",
        "    x = x - np.max(x, axis=1, keepdims=True)\n",
        "    ex = np.exp(x)\n",
        "    return ex / np.sum(ex, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(probs, labels_onehot):\n",
        "    # probs: (N, C), labels_onehot: (N, C)\n",
        "    N = probs.shape[0]\n",
        "    clipped = np.clip(probs, 1e-12, 1.0)\n",
        "    loss = -np.sum(labels_onehot * np.log(clipped)) / N\n",
        "    return loss\n",
        "\n",
        "def softmax_cross_entropy_backward(probs, labels_onehot):\n",
        "    # derivative of loss w.r.t logits pre-softmax is (probs - labels)/N\n",
        "    N = probs.shape[0]\n",
        "    return (probs - labels_onehot) / N\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 8] ScratchLeNetClassifier (versión moderna con ReLU y MaxPool)\n",
        "# Arquitectura:\n",
        "# Conv(1->6,5x5,stride1) -> ReLU -> MaxPool(2)\n",
        "# Conv(6->16,5x5,stride1) -> ReLU -> MaxPool(2)\n",
        "# Flatten\n",
        "# FC 120 -> ReLU\n",
        "# FC 84 -> ReLU\n",
        "# FC 10 -> Softmax\n",
        "# ----------------------------\n",
        "class ScratchLeNetClassifier:\n",
        "    def __init__(self, lr=1e-2):\n",
        "        self.conv1 = Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
        "        self.conv2 = Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        # compute flatten size for MNIST 28x28:\n",
        "        # After conv1: (28-5+1)=24 -> pool -> 12\n",
        "        # After conv2: (12-5+1)=8 -> pool -> 4 => 16 channels * 4 * 4 = 256\n",
        "        fc_in = 16 * 4 * 4\n",
        "        self.fc1 = Linear(fc_in, 120)\n",
        "        self.fc2 = Linear(120, 84)\n",
        "        self.fc3 = Linear(84, 10)\n",
        "        self.lr = lr\n",
        "        # caches for activations to apply relu backward\n",
        "        self.caches = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, C=1, H=28, W=28)\n",
        "        a1 = self.conv1.forward(x)\n",
        "        r1 = relu(a1)\n",
        "        p1 = self.pool1.forward(r1)\n",
        "        a2 = self.conv2.forward(p1)\n",
        "        r2 = relu(a2)\n",
        "        p2 = self.pool2.forward(r2)\n",
        "        flat = self.flatten.forward(p2)\n",
        "        z1 = self.fc1.forward(flat)\n",
        "        r3 = relu(z1)\n",
        "        z2 = self.fc2.forward(r3)\n",
        "        r4 = relu(z2)\n",
        "        logits = self.fc3.forward(r4)\n",
        "        probs = softmax(logits)\n",
        "        # store caches for backward\n",
        "        self.caches = dict(a1=a1, r1=r1, p1=p1, a2=a2, r2=r2, p2=p2, flat=flat, z1=z1, r3=r3, z2=z2, r4=r4, logits=logits, probs=probs)\n",
        "        return probs\n",
        "\n",
        "    def backward(self, labels_onehot):\n",
        "        probs = self.caches['probs']\n",
        "        logits = self.caches['logits']\n",
        "        # dlogits\n",
        "        dlogits = softmax_cross_entropy_backward(probs, labels_onehot)  # (N,10)\n",
        "        # fc3 backward\n",
        "        dr4 = self.fc3.backward(dlogits)  # (N,84)\n",
        "        # fc3 grads stored in layer\n",
        "        dz2 = relu_backward(self.caches['z2'], dr4)\n",
        "        dr3 = self.fc2.backward(dz2)\n",
        "        dz1 = relu_backward(self.caches['z1'], dr3)\n",
        "        dflat = self.fc1.backward(dz1)\n",
        "        # flatten backward\n",
        "        dp2 = self.flatten.backward(dflat)\n",
        "        # pool2 backward\n",
        "        dr2 = self.pool2.backward(dp2)\n",
        "        da2 = relu_backward(self.caches['a2'], dr2)\n",
        "        dp1 = self.conv2.backward(da2)\n",
        "        dr1 = self.pool1.backward(dp1)\n",
        "        da1 = relu_backward(self.caches['a1'], dr1)\n",
        "        dx = self.conv1.backward(da1)\n",
        "        # after backward, update weights\n",
        "        # step for conv and fc layers\n",
        "        self.conv1.step(self.lr)\n",
        "        self.conv2.step(self.lr)\n",
        "        self.fc1.step(self.lr)\n",
        "        self.fc2.step(self.lr)\n",
        "        self.fc3.step(self.lr)\n",
        "        # return loss components if needed\n",
        "        return\n",
        "\n",
        "    def compute_loss(self, probs, labels_onehot):\n",
        "        return cross_entropy_loss(probs, labels_onehot)\n",
        "\n",
        "    def predict(self, x):\n",
        "        probs = self.forward(x)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 2] Test de forward/backward en matriz pequeña (verificación)\n",
        "# Aquí replicamos el ejemplo dado y comprobamos resultados numéricos.\n",
        "# Entrada x (1,1,4,4) y filtros w (2,3,3) con los valores del enunciado.\n",
        "# ----------------------------\n",
        "def problem2_check():\n",
        "    x = np.array([[[[ 1,  2,  3,  4],\n",
        "                    [ 5,  6,  7,  8],\n",
        "                    [ 9, 10, 11, 12],\n",
        "                    [13, 14, 15, 16]]]], dtype=np.float32)  # (1,1,4,4)\n",
        "\n",
        "    # Provided w has shape (2,3,3) but for our Conv2d we need (out_channels, in_channels, kh, kw)\n",
        "    # Let's construct weight array: 2 output channels, 1 input channel, 3x3 each.\n",
        "    w = np.array([[[[ 0.,  0.,  0.],\n",
        "                    [ 0.,  1.,  0.],\n",
        "                    [ 0., -1.,  0.]]],\n",
        "\n",
        "                  [[[ 0.,  0.,  0.],\n",
        "                    [ 0., -1.,  1.],\n",
        "                    [ 0.,  0.,  0.]]]], dtype=np.float32)  # (2,1,3,3)\n",
        "    b = np.zeros((2,), dtype=np.float32)\n",
        "\n",
        "    conv = Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
        "    conv.W = w.copy()\n",
        "    conv.b = b.copy()\n",
        "\n",
        "    # forward\n",
        "    y = conv.forward(x)\n",
        "    print(\"Problema 2 - Forward output:\\n\", y.astype(int))\n",
        "    # Expected forward:\n",
        "    # array([[[-4, -4],\n",
        "    #         [-4, -4]],\n",
        "    #\n",
        "    #        [[ 1,  1],\n",
        "    #         [ 1,  1]]])\n",
        "    # Now backward with provided delta\n",
        "    delta = np.array([[[ -4,  -4],\n",
        "                       [ 10,  11]],\n",
        "\n",
        "                      [[  1,  -7],\n",
        "                       [  1, -11]]], dtype=np.float32)  # (2,2,2)\n",
        "    # delta shape needs to be (N, outC, out_h, out_w)\n",
        "    delta = delta.reshape(1, 2, 2, 2)\n",
        "    dx = conv.backward(delta)\n",
        "    print(\"Problema 2 - Backward dx (padded removed):\\n\", dx[0,0].astype(int))\n",
        "    # Expected backward (with padding) result per enunciado:\n",
        "    # array([[-5,  4],\n",
        "    #        [13, 27]])\n",
        "    # Note: The example says \"Con padding, la salida es...\" — our dx is without padding trimmed to input shape.\n",
        "    return y, dx\n",
        "\n",
        "# Run the Problem 2 check\n",
        "print(\"=== Ejecutando verificación Problema 2 ===\")\n",
        "y2, dx2 = problem2_check()\n",
        "print(\"=== Fin verificación Problema 2 ===\\n\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 7 & 8] Entrenamiento (MNIST) - entrenamos una pasada pequeña para demostrar funcionamiento.\n",
        "# NOTA: para ahorrar tiempo en Colab, usamos subset (por ejemplo N=2000) y pocas épocas.\n",
        "# ----------------------------\n",
        "def load_mnist_nchw(subset=2000):\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # Normalize floats\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "    # expand channel axis to NCHW\n",
        "    x_train = x_train[:subset]\n",
        "    y_train = y_train[:subset]\n",
        "    x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "    x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "    y_test = y_test.astype(int)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Small trainer\n",
        "def train_small(model, x_train, y_train, epochs=1, batch_size=64):\n",
        "    N = x_train.shape[0]\n",
        "    steps = max(1, N // batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        t0 = time()\n",
        "        # shuffle\n",
        "        perm = np.random.permutation(N)\n",
        "        x_train = x_train[perm]\n",
        "        y_train = y_train[perm]\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb = x_train[i:i+batch_size]\n",
        "            yb = y_train[i:i+batch_size]\n",
        "            labels_onehot = to_categorical(yb, num_classes=10)\n",
        "            probs = model.forward(xb)\n",
        "            loss = model.compute_loss(probs, labels_onehot)\n",
        "            total_loss += loss * xb.shape[0]\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "            total_correct += np.sum(preds == yb)\n",
        "            model.backward(labels_onehot)\n",
        "        avg_loss = total_loss / N\n",
        "        acc = total_correct / N\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - loss: {avg_loss:.4f} - acc: {acc*100:.2f}% - time: {time()-t0:.1f}s\")\n",
        "\n",
        "# Load subset of MNIST and train\n",
        "print(\"=== Cargando MNIST y entrenando LeNet pequeño (subset) ===\")\n",
        "x_train, y_train, x_test, y_test = load_mnist_nchw(subset=2000)\n",
        "model = ScratchLeNetClassifier(lr=1e-2)\n",
        "train_small(model, x_train, y_train, epochs=1, batch_size=128)\n",
        "# Quick test on a small portion of test set\n",
        "x_test_small = x_test[:500]\n",
        "y_test_small = y_test[:500]\n",
        "preds = model.predict(x_test_small)\n",
        "acc_test = np.mean(preds == y_test_small)\n",
        "print(f\"Test accuracy (first 500 samples): {acc_test*100:.2f}%\")\n",
        "print(\"=== Fin entrenamiento y evaluación rápida ===\\n\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 10] Cálculo del tamaño de salida y número de parámetros\n",
        "# Tenemos 3 casos, calculamos y mostramos.\n",
        "# Fórmula para parámetros de una conv2d:\n",
        "# params = (in_channels * filter_h * filter_w) * out_channels + out_channels (biases)\n",
        "# ----------------------------\n",
        "def problem10_calculations():\n",
        "    cases = [\n",
        "        dict(in_size=(144,144), in_ch=3, filter=(3,3), out_ch=6, stride=1, pad=0),\n",
        "        dict(in_size=(60,60), in_ch=24, filter=(3,3), out_ch=48, stride=1, pad=0),\n",
        "        dict(in_size=(20,20), in_ch=10, filter=(3,3), out_ch=20, stride=2, pad=0),\n",
        "    ]\n",
        "    results = []\n",
        "    for case in cases:\n",
        "        Hin, Win = case['in_size']\n",
        "        Fh, Fw = case['filter']\n",
        "        Sh = case['stride']\n",
        "        Sw = case['stride']\n",
        "        P = case['pad']\n",
        "        out_h, out_w = conv2d_output_size(Hin, Win, Fh, Fw, P, P, Sh, Sw)\n",
        "        # If non-integer (i.e., leftover pixels), formula uses floor via integer division above.\n",
        "        params = (case['in_ch'] * Fh * Fw) * case['out_ch'] + case['out_ch']\n",
        "        results.append((case, out_h, out_w, params))\n",
        "    return results\n",
        "\n",
        "res10 = problem10_calculations()\n",
        "print(\"=== Problema 10: Tamaño de salida y número de parámetros ===\")\n",
        "for case, oh, ow, params in res10:\n",
        "    Hin, Win = case['in_size']\n",
        "    print(f\"Input: {Hin}x{Win}, in_ch={case['in_ch']}, filter={case['filter']}, out_ch={case['out_ch']}, stride={case['stride']}\")\n",
        "    print(f\" -> Output size: {oh} x {ow}; Num params (incl. bias): {params}\")\n",
        "print(\"=== Fin Problema 10 ===\\n\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 9] (Investigación) - RESUMEN\n",
        "# (Se hará un breve resumen más abajo con citas web.run)\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# [Problema 11] Investigación sobre tamaño de filtro - respuestas dentro de comentarios\n",
        "# Se proveerán las respuestas teóricas en bloque multilinea debajo (como exige el enunciado).\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# [Problemas 9, 10, 11] - Respuestas teóricas (incluidas como comentarios multilínea)\n",
        "# ----------------------------\n",
        "PROBLEM_9_10_11_TEXT = \"\"\"\n",
        "[Problema 9] Resumen rápido (ver referencias externas):\n",
        "- AlexNet (Krizhevsky et al., 2012): introdujo un gran CNN entrenado en ImageNet con ReLUs, dropout, y entrenamiento en GPU multi-GPU. Aceleró la adopción de redes profundas en visión por su gran mejora en ILSVRC-2012.\n",
        "- VGG16 (Simonyan & Zisserman, 2014): mostró que apilar muchas capas de convolución 3x3 (profundidad 16-19) mejora la precisión; diseño muy simple y repetitivo (bloques de conv3x3 + maxpool), usado ampliamente como extractor de características preentrenado.\n",
        "\n",
        "[Problema 10] (Cálculos realizados por el código):\n",
        "Resultados mostrados en la salida del programa:\n",
        "- Caso 1 -> Output size y número de parámetros (incl. bias).\n",
        "- Caso 2 -> ...\n",
        "- Caso 3 -> ...\n",
        "(Ver impresión en la ejecución).\n",
        "\n",
        "[Problema 11] Sobre tamaños de filtro:\n",
        "- Por qué 3x3 en lugar de 7x7:\n",
        "  * Las pilas de 2 o 3 filtros 3x3 consecutivos tienen la misma recepción de campo que un filtro mayor (por ejemplo, tres 3x3 -> RF 7x7) pero con menos parámetros y más no linealidad (más ReLU), lo cual mejora la capacidad de representación y reduce el número de parámetros.\n",
        "  * 3x3 es un buen equilibrio entre captar contexto local y mantener eficiencia computacional y de memoria.\n",
        "- Efecto de un filtro 1x1:\n",
        "  * Un 1x1 actúa como combinación lineal across-channels (mezcla de canales) sin afectar la dimensión espacial.\n",
        "  * Es muy útil para cambiar el número de canales (dimension-reduction/expansion), añadir no-linealidad entre convoluciones espaciales, y como \"bottleneck\" para reducir parámetros (ej.: ResNet/Bottleneck).\n",
        "\"\"\"\n",
        "# (lo anterior queda como documentación; no se imprime de nuevo aquí)\n",
        "\n",
        "# Guardamos resultado para posible inspección\n",
        "__PROBLEM_SUMMARY = PROBLEM_9_10_11_TEXT\n",
        "\n",
        "# ----------------------------\n",
        "# FIN DEL SCRIPT\n",
        "# ----------------------------\n",
        "# El script implementa Conv2d, MaxPool2D, AveragePool2D, Flatten, la arquitectura LeNet (ScratchLeNetClassifier),\n",
        "# y resuelve los problemas de verificación (Problema 2), cálculo de tamaños/parametros (Problema 10),\n",
        "# y proporciona respuestas teóricas (Problema 9 y 11) dentro de la variable PROBLEM_9_10_11_TEXT.\n",
        "#\n",
        "# Para ejecutar todo en Colab: copia y pega este archivo en una celda y ejecútalo.\n",
        "# Puedes ajustar subset en load_mnist_nchw(subset=...) y epochs en train_small(...) para experimentar más.\n"
      ]
    }
  ]
}